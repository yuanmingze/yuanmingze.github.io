<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta name="keywords" content="Mingze Yuan">
  <meta name="description" content="Academic homepage of Mingze Yuan">
  <META NAME="DESCRIPTION" CONTENT="袁铭择，巴斯大学">
  <META NAME="DESCRIPTION" CONTENT="Mingze Yuan,University of Bath">

  <link rel="shortcut icon" href="./img/icon.png">
  <link href="main.css" media="all" rel="stylesheet">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-137280818-1"></script>
  <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-137280818-1');
  </script>

  <script type="text/javascript" src="./js/hidebib.js"></script>
  <base target="_blank">
  <title>Mingze Yuan's Homepage</title>
</head>
<body>

<table id="tb_info" width="100%">
  <tr>
    <td rowspan="2" align="center">
      <img src="./img/mingzeyuan.jpg" border="0" height="180" class="portrait">
    </td>
    <td width="500">
      <h1>Mingze Yuan</h1>
    </td>
  </tr>
  <tr>
    <td>
      <div style="margin:0px 0px 10px 0px;">
        Research Software Engineer<br>
        <a href="https://www.bath.ac.uk/">University of Bath</a><br>
      </div>
      <div style="clear:both">
      <div style="margin:0px 0px 10px 0px;">
            <div style="float:left;">
              <strong>Email:</strong>
            </div>
            <div style="float:left;">
              <div >yuanmingze2014 at gmail dot com</div>
            </div>
      </div>
      <div style="clear:both">
      <div class="content">
        <strong><a href="https://scholar.google.com.ph/citations?user=LDFSAdUAAAAJ">Google Scholar</a></strong> |
        <strong><a href="./file/CV_Mingze_Yuan.pdf">CV</a></strong> |
        <strong><a href="./file/Research_statement_Ming-Ze_Yuan_Post-Doc_UCAS.pdf">Research Statement</a></strong>
      </div>
    </td>
  </tr>
</table>

<div class="autobiography">
  Mingze Yuan is a Research Software Engineer working on  the Visual Computing Group at the University of Bath with <a href="https://richardt.name/">Christian Richardt</a>. 
  <br>  <br>
  Mingze graduated with a PhD from <a href="http://humanmotion.ict.ac.cn/en/index.html">Human Motion Research Group</a> at <a href="https://english.ict.ac.cn">Institute of Computing Technology (ICT)</a>, <a href="https://english.cas.cn">Chinese Academy of Sciences (CAS)</a>, where he was fortunately advised by Prof. <strong><a href="http://people.ucas.ac.cn/~0004712?language=en" rel="nofollow" class="caption-2">Shihong Xia</a></strong>, and also collaborated with Prof. <strong><a href="http://www.geometrylearning.com/" rel="nofollow" class="caption-2">Lin Gao</a></strong>.
  <br>
  He received his M.E. and B.E. degrees in computer science from <a href="http://en.cetc.com.cn">the 15th Research Institute of China Electronics Technology Group Corporation (CETC 15)</a> and <a href="https://en.uestc.edu.cn">University of Electronic Science and Technology of China (UESTC)</a> in 2011 and 2007, respectively.
</div>

<div>
  <hr>
</div>

<p id="publications" class="title-large">Publications</p>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

  <tr>
    <td width="20%" valign="top">
      <a href="./file/papersiggraphasia2020/teaser_full.jpg" class="hoverZoomLink">
        <img class="papericon" src="./file/papersiggraphasia2020/teaser.jpg" alt="papersiggraphasia2020" width="100%" border="1">
      </a>
    <td width="80%" valign="top">
      <p class="title-small">OmniPhotos: Casual 360° VR Photography</p>
      <p class="margin-small">&nbsp;</p>
      <p class="content">Tobias Bertel, <strong>Mingze Yuan</strong>, Reuben Lindroos, <a href="https://richardt.name/">Christian Richardt</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>ACM Transactions on Graphics(SIGGRAPH Asia 2020)</em>(<strong>TOG</strong>), 2020</p>
      <p class="margin-small">&nbsp;</p>
      <div class="paper" id="siggraphasia2020">
        <p class="content">
          <strong><a href="https://dl.acm.org/doi/10.1145/3415255.3422884">paper</a></strong> |
          <strong><a href="https://richardt.name/publications/omniphotos/">project</a></strong> |
          <strong><a href="#" onclick="toggleblock('siggraphasia2020_abs');return false;">abstract</a></strong> |
          <strong><a href="#" onclick="togglebib('siggraphasia2020');return false;">bibtex</a></strong>
        </p>
        <p align="justify" > <i id="siggraphasia2020_abs" style="border: 1px dotted #000000;margin:20px;padding:10px">Until now, immersive 360° VR panoramas could not be captured casually and reliably at the same time as state-of-the-art approaches involve time-consuming or expensive capture processes that prevent the casual capture of real-world VR environments. Existing approaches are also often limited in their supported range of head motion. We introduce OmniPhotos, a novel approach for casually and reliably capturing high-quality 360° VR panoramas. Our approach only requires a single sweep of a consumer 360° video camera as input, which takes less than 3 seconds with a rotating selfie stick. The captured video is transformed into a hybrid scene representation consisting of a coarse scene-specific proxy geometry and optical flow between consecutive video frames, enabling 5-DoF real-world VR experiences. The large capture radius and 360° field of view significantly expand the range of head motion compared to previous approaches. Among all competing methods, ours is the simplest, and fastest by an order of magnitude. We have captured more than 50 OmniPhotos and show video results for a large variety of scenes. We will make our code and datasets publicly available.</i>
        </p>
        <pre xml:space="preserve" style="display: none;">
@article{OmniPhotos,
  author    = {Tobias Bertel and Mingze Yuan and Reuben Lindroos and Christian Richardt},
  title     = {{OmniPhotos}: Casual 360° {VR} Photography},
  journal   = {ACM Transactions on Graphics},
  year      = {2020},
  volume    = {39},
  number    = {6},
  pages     = {266:1--12},
  month     = dec,
  issn      = {0730-0301},
  doi       = {10.1145/3414685.3417770},
  url       = {https://richardt.name/omniphotos/},
}
        </pre>
      </div>
    </td>
  </tr>
  

  <tr>
    <td width="20%" valign="top">
      <a href="./file/papertvcg2018temporla/teaserimage_full.png" class="hoverZoomLink">
        <img class="papericon" src="./file/papertvcg2018temporla/teaserimage_small.png" alt="papertvcg2018temporla" width="100%" border="1">
      </a>
    <td width="80%" valign="top">
      <p class="title-small">Temporal Upsampling of Depth Maps Using a Hybrid Camera</p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><strong>Mingze Yuan</strong>, <a href="http://humanmotion.ict.ac.cn/en/people/gaolin.html">Lin Gao</a>, <a href="http://sweb.cityu.edu.hk/hongbofu/">Hongbo Fu</a>, <a href="http://humanmotion.ict.ac.cn/en/people/xiashihong.html">Shihong Xia</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em> IEEE Transactions on Visualization and Computer Graphics(<strong>TVCG</strong>)</em>, 2019</p>
      <p class="margin-small">&nbsp;</p>
      <div class="paper" id="papertvcg2018temporla">
        <p class="content">
          <strong><a href="https://doi.org/10.1109/TVCG.2018.2812879">paper</a></strong> |
          <strong><a href="https://youtu.be/EeAPvH4-plw">YouTube</a></strong> |
          <strong><a href="#" onclick="toggleblock('papertvcg2018temporla_abs');return false;">abstract</a></strong> |
          <strong><a href="#" onclick="togglebib('papertvcg2018temporla');return false;" class="togglebib">bibtex</a></strong>
        </p>
        <p align="justify" > <i id="papertvcg2018temporla_abs" style="border: 1px dotted #000000;margin:20px;padding:10px"> In recent years, consumer-level depth cameras have been adopted for various applications. However, they often produce depth maps at only a moderately high frame rate (approximately 30 frames per second), preventing them from being used for applications such as digitizing human performance involving fast motion. On the other hand, low-cost, high-frame-rate video cameras are available. This motivates us to develop a hybrid camera that consists of a high-frame-rate video camera and a low-frame-rate depth camera and to allow temporal interpolation of depth maps with the help of auxiliary color images. To achieve this, we develop a novel algorithm that reconstructs intermediate depth maps and estimates scene flow simultaneously. We test our algorithm on various examples involving fast, non-rigid motions of single or multiple objects. Our experiments show that our scene flow estimation method is more precise than a tracking-based method and the state-of-the-art techniques.</i></p>
        <pre xml:space="preserve" style="display: none;">
@article{yuan2018temporal,
  title={Temporal Upsampling of Depth Maps Using a Hybrid Camera},
  author={Yuanze and Gao, Lin and Fu, Hongbo and Xia, Shihong},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2018},
  publisher={IEEE}
}
        </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="20%" valign="top">
        <a href="./file/paperbmvc2018sfnet/teaserimage_full.jpg" class="hoverZoomLink">
        <img class="papericon" src="./file/paperbmvc2018sfnet/teaserimage_small.jpg" alt="paperbmvc2018sfnet" width="100%" border="1">
        </a>
    <td width="80%" valign="top">
      <p class="title-small">SF-Net: Learning Scene Flow from RGB-D Images with CNNs</p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><a href="http://ylqiao.net/">Yi-Ling Qiao</a>, <a href="http://humanmotion.ict.ac.cn/en/people/gaolin.html">Lin Gao</a>, <a href="https://users.cs.cf.ac.uk/Yukun.Lai/">Yukun Lai</a>, <a href="https://ecs.victoria.ac.nz/Main/FanglueZhang">Fang-Lue Zhang</a>, <strong>Mingze Yuan</strong>, <a href="http://humanmotion.ict.ac.cn/en/people/xiashihong.html">Shihong Xia</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>29TH British Machine Vision Conference (<strong>BMVC</strong>)</em>, 2018</p>
      <p class="margin-small">&nbsp;</p>
      <div class="paper" id="paperbmvc2018sfnet">
        <p class="content">
          <strong><a href="http://bmvc2018.org/contents/papers/1095.pdf">paper</a></strong> |
          <strong><a href="#" onclick="toggleblock('paperbmvc2018sfnet_abs');return false;">abstract</a></strong> |
          <strong><a href="#" onclick="togglebib('paperbmvc2018sfnet');return false;" class="togglebib">bibtex</a></strong>
        </p>
        <p align="justify"> <i id="paperbmvc2018sfnet_abs" style="border: 1px dotted #000000;margin:20px;padding:10px"> With the rapid development of depth sensors, RGB-D data has become much more accessible. Scene flow is one of the fundamental ways to understand the dynamic content in RGB-D image sequences. Traditional approaches estimate scene flow using registration and smoothness or local rigidity priors, which is slow and prone to errors when the priors are not fully satisfied. To address such challenges, learning based methods provide an attractive alternative. However, trivially applying CNN-based optical flow estimation methods does not produce satisfactory results. How to use deep learning to improve the estimation of scene flow from RGB-D images remains unexplored. In this work, we propose a novel learning based framework to estimate scene flow, which takes both brightness and scene flow losses. Given a pair of RGB-D images, the brightness loss is used to measure the disparity between the first RGB-D image and the deformed second RGB-D image using the scene flow, and the scene flow loss is used to learn from the ground truth of scene flow. We build a convolutional neural network to simultaneously optimize both losses. Extensive experiments on both synthetic and real-world datasets show that our method is significantly faster than existing methods and outperforms stateof- the-art real-time methods in accuracy. </i></p>
        <pre xml:space="preserve" style="display: none;">
@inproceedings{qiao2018sf,
  title={SF-Net: Learning scene flow from RGB-D images with CNNs},
  author={Qiao, Yi-Ling and Gao, Lin and Lai, Yukun and 
    Zhang, Fang-Lue and Yuan, Mingze and Xia, Shihong},
  year={2018-29th British Machine Vision Conference},
  booktitle={BMVC},
  year={2015}
}
        </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="20%" valign="top">
      <a href="./file/paperjcst2017survey/teaserimage_full.png" class="hoverZoomLink">
        <img class="papericon" src="./file/paperjcst2017survey/teaserimage_small.png" alt="paperjcst2017survey" width="100%" border="1">
      </a>
    <td width="80%" valign="top">
      <p class="title-small">A Survey on Human Performance Capture and Animation</p>
      <p class="margin">&nbsp;</p>
      <p class="content">
        <a href="http://humanmotion.ict.ac.cn/en/people/xiashihong.html">Shihong Xia</a>,
        <a href="http://humanmotion.ict.ac.cn/en/people/gaolin.html">Lin Gao</a>,
        <a href="https://users.cs.cf.ac.uk/Yukun.Lai/">Yukun Lai</a>,
        <strong>Mingze Yuan</strong>,
        <a href="http://faculty.cs.tamu.edu/jchai/">Jinxiang Chai</a>
      </p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>Journal of Computer Science and Technology (<strong>JCST</strong>)</em>, 2017</p>
      <p class="margin">&nbsp;</p>
      <div class="paper" id="paperjcst2017survey">
        <p class="content">
          <strong><a href="http://humanmotion.ict.ac.cn/papers/2017P3_ASHPCA/A%20Survey%20on%20Human%20Performance%20Capture%20and%20Animation.pdf">paper</a></strong> |
          <strong><a href="#" onclick="toggleblock('paperjcst2017survey_abs');return false;">abstract</a></strong> |
          <strong><a href="#" onclick="togglebib('paperjcst2017survey');return false;" class="togglebib">bibtex</a></strong>
        </p>
        <p align="justify"> <i id="paperjcst2017survey_abs" style="border: 1px dotted #000000;margin:20px;padding:10px"> With the rapid development of computing technology, three-dimensional (3D) human body models and their dynamic motions are widely used in the digital entertainment industry. Human performance mainly involves human body shapes and motions. Key research problems in human performance animation include how to capture and analyze static geometric appearance and dynamic movement of human bodies, and how to simulate human body motions with physical effects. In this survey, according to the main research directions of human body performance capture and animation, we summarize recent advances in key research topics, namely human body surface reconstruction, motion capture and synthesis, as well as physics-based motion simulation, and further discuss future research problems and directions. We hope this will be helpful for readers to have a comprehensive understanding of human performance capture and animation. </i></p>
        <pre xml:space="preserve" style="display: none;">
@article{xia2017survey,
  title={A survey on human performance capture and animation},
  author={Xia, Shihong and Gao, Lin and Lai, Yu-Kun and Yuan, Mingze and Chai, Jinxiang},
  journal={Journal of Computer Science and Technology},
  volume={32},
  number={3},
  pages={536--554},
  year={2017},
  publisher={Springer}
}
        </pre>
      </div>
    </td>
  </tr>
      </table>

<script xml:space="preserve" language="JavaScript">
hideblock('siggraphasia2020_abs');
hideblock('papertvcg2018temporla_abs');
hideblock('paperbmvc2018sfnet_abs');
hideblock('paperjcst2017survey_abs');
</script>

<!--<div style="display:none">
&lt;!&ndash; GoStats JavaScript Based Code &ndash;&gt;
<script type="text/javascript" src="./files/counter.js.download"></script>
  <script language="javascript">var _go_js="1.0";</script>
  <script language="javascript1.1">_go_js="1.1";</script>
  _go_js="1.2";</script><script language="javascript1.3">_go_js="1.3";</script>
  <script language="javascript1.4">_go_js="1.4";</script>
  <script language="javascript1.5">_go_js="1.5";</script>
  <script language="javascript1.6">_go_js="1.6";</script>
  <script language="javascript1.7">_go_js="1.7";</script>
  <script language="javascript1.8">_go_js="1.8";</script>
  <script language="javascript1.9">_go_js="1.9";</script>
  <script language="javascript"></script>
<script type="text/javascript">_gos='c3.gostats.com';_goa=390583;
_got=4;_goi=1;_goz=0;_god='hits';_gol='web page statistics from GoStats';_GoStatsRun();</script><a target="_blank" href="http://gostats.com/" title="web page statistics from GoStats"><img id="_go_render_39058369" alt="web page statistics from GoStats" title="web page statistics from GoStats" border="0" style="border-width:0px" src="./files/count"></a>
&lt;!&ndash; <a target="_blank" title="web page statistics from GoStats"
href="http://gostats.com"> &ndash;&gt;
</div>-->
<!--
<img alt="web page statistics from GoStats"
src="http://c3.gostats.com/bin/count/a_390583/t_4/i_1/z_0/show_hits/counter.png"
style="border-width:0" />
</a>
-->
<!-- End GoStats JavaScript Based Code -->

</body>
</html>
