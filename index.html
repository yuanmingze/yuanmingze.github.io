<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta name="keywords" content="Ming-Ze Yuan">
  <meta name="description" content="Academic homepage of Ming-Ze Yuan">
  <META NAME="DESCRIPTION" CONTENT="袁铭择，深度学习，中国科学院大学，计算技术研究所">
  <META NAME="DESCRIPTION" CONTENT="Ming-Ze Yuan,Deep Learning,UCAS,ICT">

  <link rel="shortcut icon" href="./img/icon.png">
  <link href="main.css" media="all" rel="stylesheet">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-137280818-1"></script>
  <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-137280818-1');
  </script>

  <script type="text/javascript" src="js/hidebib.js"></script>
  <base target="_blank">
  <title>Ming-Ze Yuan's Homepage</title>
</head>
<body>

<table id="tb_info" width="100%">
  <tr>
    <td rowspan="2" align="center">
      <img src="img/mingzeyuan.jpg" border="0" height="180" class="portrait">
    </td>
    <td width="500">
      <h1>Ming-Ze Yuan</h1>
    </td>
  </tr>
  <tr>
    <td>
      <p >
        Ph.D. Student<br>
        <a href="http://english.ucas.ac.cn/">University of Chinese Academy of Sciences (UCAS)</a><br>
        <a href="http://english.ict.cas.cn/">The Institute of Computing Technology of the Chinese
          Academy of Sciences (ICT, CAS)</a>
      </p>
      <br>
      <p><strong>Email: </strong> yuanmingze at ict dot ac dot cn</p>
      <br>
      <p class="content">
        <!--<strong><a href="https://github.com/yuanmingze/">GitHub</a></strong> |-->
        <strong><a href="https://scholar.google.com.ph/citations?user=LDFSAdUAAAAJ">Google Scholar</a></strong> |
        <strong><a href="./file/ming-ze_yuan_cv.pdf">CV</a></strong>
      </p>
    </td>
  </tr>
</table>

<div class="autobiography">
  I am a Ph.D. Candidate student in <a href="http://humanmotion.ict.ac.cn/en/index.html">Human Motion Research Group</a> at <a href="https://english.ict.ac.cn">Institute of Computing Technology (ICT)</a>, <a href="https://english.cas.cn">Chinese Academy of Sciences (CAS)</a>, where I am fortunately advised by Prof. <strong><a href="http://people.ucas.ac.cn/~0004712?language=en" rel="nofollow" class="caption-2">Shihong Xia</a></strong>. I also collaborate with Prof. <strong><a href="http://www.geometrylearning.com/" rel="nofollow" class="caption-2">Lin Gao</a></strong>.
  <br>
  I received my M.E. and B.E. degrees in computer science from <a href="http://en.cetc.com.cn">the 15th Research Institute of China Electronics Technology Group Corporation (CETC 15)</a> and <a href="https://en.uestc.edu.cn">University of Electronic Science and Technology of China (UESTC)</a> in 2011 and 2007, respectively.
  <br>  <br>
  I have broad interest in Computer Vision, Computer Graphics and Machine Learning, with focuses on scene flow, optical flow, image processing and neural networks.
  <br>
  <strong>Currently I am looking for a position of Post-Doc. I expect to receive Ph.D. degree in July 2019.</strong>
</div>

<div>
  <hr>
</div>

<p id="publications" class="title-large">Publications</p>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

  <tr>
    <td width="20%" valign="top">
      <a href="./file/papertvcg2018temporla/teaserimage_full.png" class="hoverZoomLink">
        <img class="papericon" src="./file/papertvcg2018temporla/teaserimage_small.png" alt="papertvcg2018temporla" width="100%" border="1">
      </a>
    <td width="80%" valign="top">
      <p class="title-small">Temporal Upsampling of Depth Maps Using a Hybrid Camera</p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><strong>Ming-Ze Yuan</strong>, <a href="http://humanmotion.ict.ac.cn/en/people/gaolin.html">Lin Gao</a>, <a href="http://sweb.cityu.edu.hk/hongbofu/">Hongbo Fu</a>, <a href="http://humanmotion.ict.ac.cn/en/people/xiashihong.html">Shihong Xia</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em> IEEE Transactions on Visualization and Computer Graphics(<strong>TVCG</strong>)</em>, 2019</p>
      <p class="margin-small">&nbsp;</p>
      <div class="paper" id="papertvcg2018temporla">
        <p class="content">
          <strong><a href="https://doi.org/10.1109/TVCG.2018.2812879">paper</a></strong> |
          <strong><a href="https://youtu.be/EeAPvH4-plw">YouTube</a></strong> |
          <strong><a href="javascript:toggleblock('papertvcg2018temporla_abs')">abstract</a></strong> |
          <strong><a shape="rect" href="javascript:togglebib('papertvcg2018temporla')" class="togglebib">bibtex</a></strong>
        </p>
        <p align="justify" > <i id="papertvcg2018temporla_abs" style="border: 1px dotted #000000;margin:20px;padding:10px"> In recent years, consumer-level depth cameras have been adopted for various applications. However, they often produce depth maps at only a moderately high frame rate (approximately 30 frames per second), preventing them from being used for applications such as digitizing human performance involving fast motion. On the other hand, low-cost, high-frame-rate video cameras are available. This motivates us to develop a hybrid camera that consists of a high-frame-rate video camera and a low-frame-rate depth camera and to allow temporal interpolation of depth maps with the help of auxiliary color images. To achieve this, we develop a novel algorithm that reconstructs intermediate depth maps and estimates scene flow simultaneously. We test our algorithm on various examples involving fast, non-rigid motions of single or multiple objects. Our experiments show that our scene flow estimation method is more precise than a tracking-based method and the state-of-the-art techniques.</i></p>
        <pre xml:space="preserve" style="display: none;">
@article{yuan2018temporal,
  title={Temporal Upsampling of Depth Maps Using a Hybrid Camera},
  author={Yuan, Ming-Ze and Gao, Lin and Fu, Hongbo and Xia, Shihong},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2018},
  publisher={IEEE}
}
        </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="20%" valign="top">
        <a href="./file/paperbmvc2018sfnet/teaserimage_full.jpg" class="hoverZoomLink">
        <img class="papericon" src="./file/paperbmvc2018sfnet/teaserimage_small.jpg" alt="paperbmvc2018sfnet" width="100%" border="1">
        </a>
    <td width="80%" valign="top">
      <p class="title-small">SF-Net: Learning Scene Flow from RGB-D Images with CNNs</p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><a href="http://ylqiao.net/">Yi-Ling Qiao</a>, <a href="http://humanmotion.ict.ac.cn/en/people/gaolin.html">Lin Gao</a>, <a href="https://users.cs.cf.ac.uk/Yukun.Lai/">Yukun Lai</a>, <a href="https://ecs.victoria.ac.nz/Main/FanglueZhang">Fang-Lue Zhang</a>, <strong>Ming-Ze Yuan</strong>, <a href="http://humanmotion.ict.ac.cn/en/people/xiashihong.html">Shihong Xia</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>29TH British Machine Vision Conference (<strong>BMVC</strong>)</em>, 2018</p>
      <p class="margin-small">&nbsp;</p>
      <div class="paper" id="paperbmvc2018sfnet">
        <p class="content">
          <strong><a href="http://bmvc2018.org/contents/papers/1095.pdf">paper</a></strong> |
          <strong><a href="javascript:toggleblock('paperbmvc2018sfnet_abs')">abstract</a></strong> |
          <strong><a shape="rect" href="javascript:togglebib('paperbmvc2018sfnet')" class="togglebib">bibtex</a></strong>
        </p>
        <p align="justify"> <i id="paperbmvc2018sfnet_abs" style="border: 1px dotted #000000;margin:20px;padding:10px"> With the rapid development of depth sensors, RGB-D data has become much more accessible. Scene flow is one of the fundamental ways to understand the dynamic content in RGB-D image sequences. Traditional approaches estimate scene flow using registration and smoothness or local rigidity priors, which is slow and prone to errors when the priors are not fully satisfied. To address such challenges, learning based methods provide an attractive alternative. However, trivially applying CNN-based optical flow estimation methods does not produce satisfactory results. How to use deep learning to improve the estimation of scene flow from RGB-D images remains unexplored. In this work, we propose a novel learning based framework to estimate scene flow, which takes both brightness and scene flow losses. Given a pair of RGB-D images, the brightness loss is used to measure the disparity between the first RGB-D image and the deformed second RGB-D image using the scene flow, and the scene flow loss is used to learn from the ground truth of scene flow. We build a convolutional neural network to simultaneously optimize both losses. Extensive experiments on both synthetic and real-world datasets show that our method is significantly faster than existing methods and outperforms stateof- the-art real-time methods in accuracy. </i></p>
        <pre xml:space="preserve" style="display: none;">
@inproceedings{qiao2018sf,
  title={SF-Net: Learning scene flow from RGB-D images with CNNs},
  author={Qiao, Yi-Ling and Gao, Lin and Lai, Yukun and Zhang, Fang-Lue and
          Yuan, Ming-Ze and Xia, Shihong},
  year={2018-29th British Machine Vision Conference},
  booktitle={BMVC},
  year={2015}
}
        </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="20%" valign="top">
      <a href="./file/paperjcst2017survey/teaserimage_full.png" class="hoverZoomLink">
        <img class="papericon" src="./file/paperjcst2017survey/teaserimage_small.png" alt="paperjcst2017survey" width="100%" border="1">
      </a>
    <td width="80%" valign="top">
      <p class="title-small">A Survey on Human Performance Capture and Animation</p>
      <p class="margin">&nbsp;</p>
      <p class="content">
        <a href="http://humanmotion.ict.ac.cn/en/people/xiashihong.html">Shihong Xia</a>,
        <a href="http://humanmotion.ict.ac.cn/en/people/gaolin.html">Lin Gao</a>,
        <a href="https://users.cs.cf.ac.uk/Yukun.Lai/">Yukun Lai</a>,
        <strong>Ming-Ze Yuan</strong>,
        <a href="http://faculty.cs.tamu.edu/jchai/">Jinxiang Chai</a>
      </p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>Journal of Computer Science and Technology (<strong>JCST</strong>)</em>, 2017</p>
      <p class="margin">&nbsp;</p>
      <div class="paper" id="paperjcst2017survey">
        <p class="content">
          <strong><a href="http://humanmotion.ict.ac.cn/papers/2017P3_ASHPCA/A%20Survey%20on%20Human%20Performance%20Capture%20and%20Animation.pdf">paper</a></strong> |
          <strong><a href="javascript:toggleblock('paperjcst2017survey_abs')">abstract</a></strong> |
          <strong><a shape="rect" href="javascript:togglebib('paperjcst2017survey')" class="togglebib">bibtex</a></strong>
        </p>
        <p align="justify"> <i id="paperjcst2017survey_abs" style="border: 1px dotted #000000;margin:20px;padding:10px"> With the rapid development of computing technology, three-dimensional (3D) human body models and their dynamic motions are widely used in the digital entertainment industry. Human performance mainly involves human body shapes and motions. Key research problems in human performance animation include how to capture and analyze static geometric appearance and dynamic movement of human bodies, and how to simulate human body motions with physical effects. In this survey, according to the main research directions of human body performance capture and animation, we summarize recent advances in key research topics, namely human body surface reconstruction, motion capture and synthesis, as well as physics-based motion simulation, and further discuss future research problems and directions. We hope this will be helpful for readers to have a comprehensive understanding of human performance capture and animation. </i></p>
        <pre xml:space="preserve" style="display: none;">
@article{xia2017survey,
  title={A survey on human performance capture and animation},
  author={Xia, Shihong and Gao, Lin and Lai, Yu-Kun and Yuan, Ming-Ze and Chai, Jinxiang},
  journal={Journal of Computer Science and Technology},
  volume={32},
  number={3},
  pages={536--554},
  year={2017},
  publisher={Springer}
}
        </pre>
      </div>
    </td>
  </tr>
      </table>

<script xml:space="preserve" language="JavaScript">
hideblock('papertvcg2018temporla_abs');
hideblock('paperbmvc2018sfnet_abs');
hideblock('paperjcst2017survey_abs');
</script>

<!--<div style="display:none">
&lt;!&ndash; GoStats JavaScript Based Code &ndash;&gt;
<script type="text/javascript" src="./files/counter.js.download"></script>
  <script language="javascript">var _go_js="1.0";</script>
  <script language="javascript1.1">_go_js="1.1";</script>
  _go_js="1.2";</script><script language="javascript1.3">_go_js="1.3";</script>
  <script language="javascript1.4">_go_js="1.4";</script>
  <script language="javascript1.5">_go_js="1.5";</script>
  <script language="javascript1.6">_go_js="1.6";</script>
  <script language="javascript1.7">_go_js="1.7";</script>
  <script language="javascript1.8">_go_js="1.8";</script>
  <script language="javascript1.9">_go_js="1.9";</script>
  <script language="javascript"></script>
<script type="text/javascript">_gos='c3.gostats.com';_goa=390583;
_got=4;_goi=1;_goz=0;_god='hits';_gol='web page statistics from GoStats';_GoStatsRun();</script><a target="_blank" href="http://gostats.com/" title="web page statistics from GoStats"><img id="_go_render_39058369" alt="web page statistics from GoStats" title="web page statistics from GoStats" border="0" style="border-width:0px" src="./files/count"></a>
&lt;!&ndash; <a target="_blank" title="web page statistics from GoStats"
href="http://gostats.com"> &ndash;&gt;
</div>-->
<!--
<img alt="web page statistics from GoStats"
src="http://c3.gostats.com/bin/count/a_390583/t_4/i_1/z_0/show_hits/counter.png"
style="border-width:0" />
</a>
-->
<!-- End GoStats JavaScript Based Code -->

</body>
</html>
